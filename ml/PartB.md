# Part B Response

There are potentially multiple levels of complexity one could go into regarding finetuning an LLM to improve performance for this task. As a very naive approach one could simply finetune the model by using a next token prediction task on triplets consisting of: system prompts, vetted user contexts from previous calls and the corresponding desired output. One could, in this case utilize, a very short system prompt (consisting of just the options and a Flan-t5 like instruction) since it will mostly serve as a marker for the task post finetuning. Additionally the desired outputs could be selected to be as short as possible such as an int index ("0", "1"...). This process would reduce the latency of the system substantially, since the inputs to the model are significantly smaller (reducing the time taken to encode the prompt and context), and the model will only need to output a single token as a response (theoretically in causal generation each token generated takes increasingly longer to generate than the previous). Neither of these outcomes would be possible without finetuning as the model would not understand the task with such a small system prompt nor would it be likely to generate just a single output token before the output could be evaluated. In addition to improving latency the accuracy of the model's responses would likely improve as it will have been trained on a number of common and ambiguous cases. As a more advanced approach one could consider techniques such as synthetic data generation for contexts using LLMs or other techniques or closely related to that: knowledge distillation into a smaller model. The only reliable way to improve the time to first byte "cet. par" would be to reduce the computational footprint of the model being used thus finetuning smaller models to outperform larger ones is the obvious solution. 